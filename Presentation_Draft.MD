In a sentence or two, the idea of Meta crawler is to crawl over the shared folder file system and network and create semantic meta tags allowing agents to more easily navigate our files and data and bring structure to the untidy mess of our files that is growing ever larger year by year and in the process turning a potential liability into the asset it should be. 
For the purpose of the hackathon we will be focusing on documents but will try to in-still an idea of what the end state system might look like. What is really exciting about this idea is that without AI agents this task is next to impossible. We have such a large backlog of untidy files and are constantly adding more makes it infeasible for humans to classify them all. 
The contents of many of these files and the fuzzy and inexact nature of trying to classify them also rules out the use of traditional classification methods. Whereas AI agents can work tirelessly night and day continuously reorganising and tidying, in much the same way that web crawlers are constantly re-indexing the web. 
We build out prototype to identify correctly classify documents and to flag all other documents for further review.
The main tags that we have focused on are Classification and Relevance. 
Relevance includes information on when it was created, last modified, version number if any and a confidence level in how likely it is to be the latest version. 
The more files that get crawled the more powerful this system becomes as the files can be cross-referenced to make sure only the newest and most relevant versions are being used. 
The more far reaching goal of the project would be to add even more meta tags to the system to work as a kind of SEO for the agents of the future to always retrieve the correct information.
The blue-sky version of this project would be to crawl over all ANZ's data adding even more and richer meta tags to the index to work as a kind of SEO for the agent of the future, always retrieving the correct information.
