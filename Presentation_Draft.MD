In a sentence or two, the idea of Meta crawler is to crawl over the file system and network and create semantic meta tags Allowing agents to more easily navigate 
our files and data and bring structure to the unttidy mess of our files that is growing ever larger year by year and in the process turning a potential laibility into the asset it should be.

For the purpose of the hackathon we will be focusing on documents but will try to instill an idea of what the end state system might look like.

What is really exciting about this idea is that without AI agents this task is next to impossible. We have such a large backlog of untidy files and are accumulating more at a rate that it makes it infeasible
to employ humans to sort through it all. The contents of many of these files and the fuzzy and inexact nature of trying to classify them also rules out the use of traditional classification methods. Whereas AI
agents can work tirelessly night and day continuously reorganising and tidying, in much the same way that web crawlers are constantly re-indexing the web. AI agents are also great at fuzzy tasks, we have built our
prototype to not only provide classifications such (public, internal, confidential or restricted) but to also provide a confidence rating for each classification [pub: 0%, int: 0%, conf: 80%, rest: 20%].

The main tags that we have focused on are Classification and Relevance. Relevance includes information on when it was created, last modified, version number if any and a confidence level in how likely it is to be 
the latest version. 

The more files that get crawled the more powerful this sytem becomes as the files can be cross-referenced to make sure only the newest and most relevant versions are being used.

The more far reaching goal of the project would be to add even more meta tags to the system to work as a kind of SEO for the agents of the future to always retrieve the correct information.
